{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e03dfd9",
   "metadata": {},
   "source": [
    "# LLM Extraction\n",
    "\n",
    "Leveraging modern LLM capabilities I want to try to extract in an structured manner all the degrees completed by the subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adfa0d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "from langchain import LLMChain, PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.llms import LlamaCpp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da42b54",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9acf681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pydantic will be used for data validation\n",
    "\n",
    "class Degree(BaseModel):\n",
    "    Degree_type: str = Field(..., description=\"e.g. 'Bachelor of Arts', 'Professional', etc.\")\n",
    "    Degree_field: List[str] = Field(..., description=\"e.g. ['Political Science'], ['jd']\")\n",
    "\n",
    "class AuthorDegrees(BaseModel):\n",
    "    id: int\n",
    "    author_name: str\n",
    "    degrees: List[Degree]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dacbebf",
   "metadata": {},
   "source": [
    "## Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a1f6942d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'...Neil Armstrong! He stepped out of the lunar module Eagle and onto the moon\\'s surface on July 20, 1969, famously declaring \"That\\'s one small step for man, one giant leap for mankind\" as he became the first person to set foot on the moon.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "\n",
    "llm = OllamaLLM(model=\"llama3.1:8b\")\n",
    "\n",
    "llm.invoke(\"The first man on the moon was ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d2813e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from ../models/llama-2-7b-chat.Q5_K_M.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 17\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q5_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "print_info: file format = GGUF V2\n",
      "print_info: file type   = Q5_K - Medium\n",
      "print_info: file size   = 4.45 GiB (5.68 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 1\n",
      "load: control token:      2 '</s>' is not marked as EOG\n",
      "load: control token:      1 '<s>' is not marked as EOG\n",
      "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "load: special tokens cache size = 3\n",
      "load: token to piece cache size = 0.1684 MB\n",
      "print_info: arch             = llama\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 4096\n",
      "print_info: n_embd           = 4096\n",
      "print_info: n_layer          = 32\n",
      "print_info: n_head           = 32\n",
      "print_info: n_head_kv        = 32\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: n_swa_pattern    = 1\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 1\n",
      "print_info: n_embd_k_gqa     = 4096\n",
      "print_info: n_embd_v_gqa     = 4096\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-06\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 11008\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 0\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 10000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 4096\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: ssm_d_conv       = 0\n",
      "print_info: ssm_d_inner      = 0\n",
      "print_info: ssm_d_state      = 0\n",
      "print_info: ssm_dt_rank      = 0\n",
      "print_info: ssm_dt_b_c_rms   = 0\n",
      "print_info: model type       = 7B\n",
      "print_info: model params     = 6.74 B\n",
      "print_info: general.name     = LLaMA v2\n",
      "print_info: vocab type       = SPM\n",
      "print_info: n_vocab          = 32000\n",
      "print_info: n_merges         = 0\n",
      "print_info: BOS token        = 1 '<s>'\n",
      "print_info: EOS token        = 2 '</s>'\n",
      "print_info: UNK token        = 0 '<unk>'\n",
      "print_info: LF token         = 13 '<0x0A>'\n",
      "print_info: EOG token        = 2 '</s>'\n",
      "print_info: max token length = 48\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors: layer   0 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   1 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   2 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   3 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   4 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   5 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   6 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   7 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   8 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   9 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  10 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  11 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  12 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  13 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  14 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  15 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  16 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  17 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  18 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  19 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  20 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  21 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  22 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  23 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  24 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  25 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  26 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  27 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  28 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  29 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  30 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  31 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  32 assigned to device CPU, is_swa = 0\n",
      "load_tensors: tensor 'token_embd.weight' (q5_K) (and 290 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "load_tensors:   CPU_Mapped model buffer size =  4560.87 MiB\n",
      "..................................................................................................\n",
      "llama_context: constructing llama_context\n",
      "llama_context: n_seq_max     = 1\n",
      "llama_context: n_ctx         = 2048\n",
      "llama_context: n_ctx_per_seq = 2048\n",
      "llama_context: n_batch       = 512\n",
      "llama_context: n_ubatch      = 512\n",
      "llama_context: causal_attn   = 1\n",
      "llama_context: flash_attn    = 0\n",
      "llama_context: freq_base     = 10000.0\n",
      "llama_context: freq_scale    = 1\n",
      "llama_context: n_ctx_per_seq (2048) < n_ctx_train (4096) -- the full capacity of the model will not be utilized\n",
      "set_abort_callback: call\n",
      "llama_context:        CPU  output buffer size =     0.12 MiB\n",
      "create_memory: n_ctx = 2048 (padded)\n",
      "llama_kv_cache_unified: kv_size = 2048, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1, padding = 32\n",
      "llama_kv_cache_unified: layer   0: dev = CPU\n",
      "llama_kv_cache_unified: layer   1: dev = CPU\n",
      "llama_kv_cache_unified: layer   2: dev = CPU\n",
      "llama_kv_cache_unified: layer   3: dev = CPU\n",
      "llama_kv_cache_unified: layer   4: dev = CPU\n",
      "llama_kv_cache_unified: layer   5: dev = CPU\n",
      "llama_kv_cache_unified: layer   6: dev = CPU\n",
      "llama_kv_cache_unified: layer   7: dev = CPU\n",
      "llama_kv_cache_unified: layer   8: dev = CPU\n",
      "llama_kv_cache_unified: layer   9: dev = CPU\n",
      "llama_kv_cache_unified: layer  10: dev = CPU\n",
      "llama_kv_cache_unified: layer  11: dev = CPU\n",
      "llama_kv_cache_unified: layer  12: dev = CPU\n",
      "llama_kv_cache_unified: layer  13: dev = CPU\n",
      "llama_kv_cache_unified: layer  14: dev = CPU\n",
      "llama_kv_cache_unified: layer  15: dev = CPU\n",
      "llama_kv_cache_unified: layer  16: dev = CPU\n",
      "llama_kv_cache_unified: layer  17: dev = CPU\n",
      "llama_kv_cache_unified: layer  18: dev = CPU\n",
      "llama_kv_cache_unified: layer  19: dev = CPU\n",
      "llama_kv_cache_unified: layer  20: dev = CPU\n",
      "llama_kv_cache_unified: layer  21: dev = CPU\n",
      "llama_kv_cache_unified: layer  22: dev = CPU\n",
      "llama_kv_cache_unified: layer  23: dev = CPU\n",
      "llama_kv_cache_unified: layer  24: dev = CPU\n",
      "llama_kv_cache_unified: layer  25: dev = CPU\n",
      "llama_kv_cache_unified: layer  26: dev = CPU\n",
      "llama_kv_cache_unified: layer  27: dev = CPU\n",
      "llama_kv_cache_unified: layer  28: dev = CPU\n",
      "llama_kv_cache_unified: layer  29: dev = CPU\n",
      "llama_kv_cache_unified: layer  30: dev = CPU\n",
      "llama_kv_cache_unified: layer  31: dev = CPU\n",
      "llama_kv_cache_unified:        CPU KV buffer size =  1024.00 MiB\n",
      "llama_kv_cache_unified: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB\n",
      "llama_context: enumerating backends\n",
      "llama_context: backend_ptrs.size() = 1\n",
      "llama_context: max_nodes = 65536\n",
      "llama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\n",
      "llama_context: reserving graph for n_tokens = 512, n_seqs = 1\n",
      "llama_context: reserving graph for n_tokens = 1, n_seqs = 1\n",
      "llama_context: reserving graph for n_tokens = 512, n_seqs = 1\n",
      "llama_context:        CPU compute buffer size =   164.01 MiB\n",
      "llama_context: graph nodes  = 1094\n",
      "llama_context: graph splits = 1\n",
      "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'general.name': 'LLaMA v2', 'general.architecture': 'llama', 'llama.context_length': '4096', 'llama.rope.dimension_count': '128', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'llama.feed_forward_length': '11008', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '2', 'general.file_type': '17', 'llama.attention.head_count_kv': '32', 'llama.attention.layer_norm_rms_epsilon': '0.000001', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '0'}\n",
      "Using fallback chat format: llama-2\n",
      "llama_perf_context_print:        load time =    3182.92 ms\n",
      "llama_perf_context_print: prompt eval time =    3181.97 ms /    48 tokens (   66.29 ms per token,    15.08 tokens per second)\n",
      "llama_perf_context_print:        eval time =   38020.89 ms /   115 runs   (  330.62 ms per token,     3.02 tokens per second)\n",
      "llama_perf_context_print:       total time =   44229.28 ms /   163 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': 'chatcmpl-bd3d7445-e600-4f00-996e-ed81e3f76bac',\n",
       " 'object': 'chat.completion',\n",
       " 'created': 1747079648,\n",
       " 'model': '../models/llama-2-7b-chat.Q5_K_M.gguf',\n",
       " 'choices': [{'index': 0,\n",
       "   'message': {'role': 'assistant',\n",
       "    'content': '{\"World Series Winners\": [\\n{\\n\"Year\": 2020,\\n\"Winner\": \"Los Angeles Dodgers\"\\n},\\n{\\n\"Year\": 2021,\\n\"Winner\": \"Atlanta Braves\"\\n},\\n{\\n\"Year\": 2022,\\n\"Winner\": \"Houston Astros\"\\n},\\n{\\n\"Year\": 2023,\\n\"Winner\": \"New York Yankees\"\\n}\\n]\\n}'},\n",
       "   'logprobs': None,\n",
       "   'finish_reason': 'stop'}],\n",
       " 'usage': {'prompt_tokens': 48, 'completion_tokens': 115, 'total_tokens': 163}}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "llm = Llama(\n",
    "    model_path=\"../models/llama-2-7b-chat.Q5_K_M.gguf\",\n",
    "    n_gpu_layers=1,\n",
    "    n_batch=512,\n",
    "    n_ctx=2048,\n",
    "    f16_kv=True,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "llm.create_chat_completion(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful assistant that outputs in JSON.\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": \"Who won the world series in from 2020 to 2023?\"},\n",
    "    ],\n",
    "    response_format={\n",
    "        \"type\": \"json_object\",\n",
    "        \"schema\": [{\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\"team_name\": {\"type\": \"string\"}},\n",
    "            \"required\": [\"team_name\"],\n",
    "        }],\n",
    "    },\n",
    "    temperature=0.7,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54826da1",
   "metadata": {},
   "source": [
    "## PydanticAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "78dbf738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id                                                             7\n",
      "author_name                                         Barack Obama\n",
      "sentences      ## Early life and career\\n- In late August 196...\n",
      "Name: 6, dtype: object\n",
      "studies=[Degree(degree_type='Bachelor of Arts', degree_field=['political science', 'international relations', 'English literature']), Degree(degree_type='Professional', degree_field=['Juris Doctor'])]\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "from pydantic_ai.models.openai import OpenAIModel\n",
    "from pydantic_ai.providers.openai import OpenAIProvider\n",
    "from pydantic_ai import Agent\n",
    "import nest_asyncio\n",
    "from httpx import AsyncClient\n",
    "from pydantic_ai.providers.deepseek import DeepSeekProvider\n",
    "import os\n",
    "import dotenv\n",
    "import pandas as pd\n",
    "dotenv.load_dotenv()\n",
    "nest_asyncio.apply()\n",
    "class Degree(BaseModel):\n",
    "    degree_type: str = Field(..., description=\"…\")\n",
    "    degree_field: List[str] = Field(..., description=\"…\")\n",
    "\n",
    "class AuthorDegrees(BaseModel):\n",
    "    studies: List[Degree]\n",
    "\n",
    "\n",
    "async def extract_degrees_async(\n",
    "\n",
    "    sentences: str,\n",
    "    model_name: str = \"llama3.1:8b\",\n",
    ") -> AuthorDegrees:\n",
    "    schema_json = AuthorDegrees.model_json_schema()\n",
    "    prompt = f\"\"\"\n",
    "Goal: Identify and categorize academic degrees from a Wikipedia text snippet. Respond only with JSON matching the AuthorDegrees schema (no extra text or markdown).:\n",
    "Steps:\n",
    "1. Scan for keywords (e.g., B.A., M.A., Bachelor, Master, Doctor).\n",
    "2. Identify degree types (e.g., Bachelor of Arts, Juris Doctor, Ph.D.) and their fields (e.g., History, Law).\n",
    "3. Use the long version of the degree name (e.g., Bachelor of Arts, Juris Doctor) and include the field of study if available.\n",
    "4. Detect incomplete degrees (“dropped out”, “did not graduate”, and similar) and ignore.\n",
    "5. If no completed degree is found, return a single entry with NONE\n",
    "6. Output schema (exact JSON):\n",
    "\n",
    "Examples:\n",
    "Q1: \"- He received his Bachelor of Arts from Harvard College in 1980.\n",
    "    - He received his Juris Doctor from the University of California at Berkeley in 1986.\"\n",
    "A1:[\n",
    "  {{\n",
    "    \"studies\": [\n",
    "      {{\n",
    "        \"degree_type\": \"Bachelor of Arts\",\n",
    "        \"degree_field\": []\n",
    "      }},\n",
    "      {{\n",
    "        \"degree_type\": \"Professional\",\n",
    "        \"degree_field\": [\"Juris Doctor\"]\n",
    "      }}\n",
    "    ]\n",
    "  }}\n",
    "]\n",
    "Q2: \"## Academic career\n",
    "- After attending University City High School in St. Louis, Missouri , Moyn earned his A.B.\n",
    "- He continued his education, earning a Ph.D. from the University of California at Berkeley (2000) and his J.D.\n",
    "\n",
    "## _infobox_education_\n",
    "- Washington University in St. Louis ( BA ) University of California, Berkeley ( PhD ) Harvard University ( JD )\n",
    "\"\n",
    "A2:[\n",
    "  {{\n",
    "    \"studies\": [\n",
    "      {{\n",
    "        \"degree_type\": \"Bachelor of Arts\",\n",
    "        \"degree_field\": [\"History\", \"French literature\"]\n",
    "      }},\n",
    "      {{\n",
    "        \"degree_type\": \"Ph.D.\",\n",
    "        \"degree_field\": []\n",
    "      }},\n",
    "      {{\n",
    "        \"degree_type\": \"Professional\",\n",
    "        \"degree_field\": [\"Juris Doctor\"]\n",
    "      }}\n",
    "    ]\n",
    "  }}\n",
    "]\n",
    "Q3: \"## Early life\n",
    "- Foster graduated from Falmouth Academy in 1994.\n",
    "\"\n",
    "A:[\n",
    "  {{\n",
    "    \"studies\": [\n",
    "      {{\n",
    "        \"degree_type\": \"NONE\",\n",
    "        \"degree_field\": []\n",
    "      }}\n",
    "    ]\n",
    "  }}\n",
    "]\n",
    "\n",
    "Q:\"{sentences}\"\n",
    "\"\"\"\n",
    "\n",
    "    ollama_model = OpenAIModel(\n",
    "        model_name=model_name,\n",
    "        provider=OpenAIProvider(base_url='http://localhost:11434/v1'),\n",
    "    )\n",
    "    \n",
    "    custom_http_client = AsyncClient(timeout=30)\n",
    "    deepseek_model = OpenAIModel(\n",
    "    'deepseek-chat',\n",
    "    provider=DeepSeekProvider(\n",
    "        api_key=os.getenv(\"DEEPSEEK_API_KEY\"), http_client=custom_http_client\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    agent = Agent(deepseek_model, output_type=AuthorDegrees, max_result_retries=3)\n",
    "    # This is an async call we can await\n",
    "    result: AuthorDegrees = await agent.run(prompt)\n",
    "    return result\n",
    "\n",
    "# In a notebook or any async-capable REPL, you can just:\n",
    "# >>> result = await extract_degrees_async(1, \"Harry Litman\", sample_sentences)\n",
    "# >>> print(result.json(indent=2))\n",
    "\n",
    "# Or, if you really want to wrap it for sync usage:\n",
    "def extract_degrees(*args, **kwargs):\n",
    "    return asyncio.get_event_loop().run_until_complete(\n",
    "        extract_degrees_async(*args, **kwargs)\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sample_sentences = pd.read_csv(\"../data/processed/parsed_files/parsed_results.csv\")\n",
    "    author = sample_sentences.iloc[6]\n",
    "    print(author)\n",
    "    result = await extract_degrees_async(author['sentences'])\n",
    "    print(result.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e23cdf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Degree(degree_type='Bachelor of Arts', degree_field=[]), Degree(degree_type='Professional', degree_field=['Juris Doctor'])]\n"
     ]
    }
   ],
   "source": [
    "print(result.output.studies)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
